<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <!-- Set the viewport so this responsive site displays correctly on mobile devices -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Hang Zhao, hangzhao (at) mit.edu">

    <title>Hang Zhao - MIT</title>
    <!-- Include bootstrap CSS -->
    <link href="includes/bootstrap/css/bootstrap-Hang.css" rel="stylesheet">
    <!--<link href="includes/bootstrap.min-4.css" rel="stylesheet">-->
</head>

<body>

<!-- Include jQuery and bootstrap JS plugins -->
<script src="includes/jquery/jquery-2.1.3.min.js"></script>
<script src="includes/bootstrap/js/bootstrap.min.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60734989-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Site header and navigation -->
<div class="navbar navbar-default navbar-fixed-top">
    <div class="container">
        <a href="index.html" class="navbar-brand pull-left">HOME</a>
        <button class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="glyphicon glyphicon-align-justify"></span>
        </button>
        <nav class="navbar-collapse collapse" role="navigation">
            <ul class="navbar-nav nav">
            	<li><a href="#project" class="scroll">PROJECTS</a></li>
              <li><a href="#activities">ACTIVITIES</a></li>
            	<li><a href="#publication">PUBLICATIONS</a></li>
            </ul>
        </nav>
    </div>
</div>

 <div class="jumbotron">
 	<div class="col-xs-12 col-md-12" style="margin-top: 225px; margin-left: 10%;">
 	<p><strong><font color="white">Hang Zhao</font></strong></p>
	 </div>
 </div>

<!-- Site banner -->
 <div class="container" style="margin-top: 20px;">
        <div class="col-xs-7 col-md-3" style="text-align: center; vertical-align: middle;">
          <br>
          <img src="images/profile3.jpg" class="img-circle" style="max-height: 80%; max-width: 80%;" />
          <br><br>
          <a href="https://scholar.google.com/citations?user=DmahiOYAAAAJ">Google Scholar</a>
          <br>
          <a href="https://github.com/hangzhaomit">GitHub</a>
        </div>

        <div class="col-xs-12 col-md-6" style="vertical-align: middle;"> 
          <p align="justify"> <br /> Hey, I am Hang Zhao, an assistant professor at IIIS, Tsinghua University. My research interests are <strong>multi-modal machine learning, autonomous driving and computer vision</strong>.
          </p>
          
          <p align="justify">I was a Research Scientist at <a href="https://waymo.com">Waymo</a> (known as Google's self-driving project).
          Before that, I got my Ph.D. degree at <a href="http://www.mit.edu/">MIT</a> under Professor <a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a> (the Great Torralba!), and my M.S. under Professor Ramesh Raskar. Before MIT, I received my B.S. from CKC Honors College, Zhejiang University.</p>
           
          <!-- <p align="justify"> I am a <a href="https://snapresearchfs.splashthat.com/">2019 Snap Research Fellow</a>. I had great research internship experiences at Facebook Research in 2017, at MERL (Mitsubishi Electric Research Laboratories) in 2016, and at NVIDIA Research in 2015.</p> 
            <p align="justify"> I am maintaining the most popular open-source semantic segmentation repository, check it out here <a href="https://github.com/CSAILVision/semantic-segmentation-pytorch">Semantic-Segmentation-Pytorch</a>.</p>
-->        
          <p align="justify"><strong>We are building <a href="http://group.iiis.tsinghua.edu.cn/~marslab/">MARS Lab</a>, I am actively looking for PostDoc/PhD/BS students with CS/EE background to join my team. If you would like to work with me, feel free to drop me an email with your resume.</strong></p>
        </div>

        <!-- Contact Info-->
        <div class="col-xs-12 col-md-3" style="text-align: left; vertical-align: middle;">
                <h3 style="margin-top: 20px;">Contact Info</h3>
                <ul>
                        <li>Address: FIT, Tsinghua University</li>
                        <li>E-mail: ZhaoHang0124 (at) gmail.com</li>
                </ul>
                  </div>

      <!-- 
      <div class="col-md-10" style="vertical-align: middle;margin-top: 10px;">
        <p>[News] I am co-organizing the <a href="http://sightsound.org">Sight and Sound Workshop</a> at CVPR'20.</p>
      </div>
      -->
</div>

<!-- Middle content section -->
<div class="middle">   

	<!-- Projects -->
	<a name="project" style="visibility: hidden;"></a>
	<div class="container" style="margin-top: 0px;">
        <div class="col-md-12 content">
            <br /><h2>Selected Projects</h2>

            <!-- NeuralDubber -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/neural_dubber.png" width="250" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>Neural Dubber: Dubbing for Videos According to Scripts</strong> <span class="label label-danger">Hot</span><br> 
                Chenxu Hu, Qiao Tian, Tingle Li, Yuping Wang, Yuxuan Wang, <strong>Hang Zhao</strong><br>
                <strong>NeurIPS 2021</strong><br>

                <i> "Automatic video dubbing driven by a neural network!" </i> <br><br>

                <a href="https://arxiv.org/abs/2110.08243" class="btn btn-default btn-sm">Paper</a>
                <a href="https://tsinghua-mars-lab.github.io/NeuralDubber/" class="btn btn-default btn-sm">Project</a>
              </div>
            </div>

            <!-- DETR3D -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/detr3d.jpeg" width="180" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries</strong> <span class="label label-danger">Hot</span><br> 
                Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, <strong>Hang Zhao</strong>, Justin Solomon<br>
                <strong>CoRL 2021</strong><br>

                <i> "A new paradigm of 3D object detection from 2D images!" </i> <br><br>

                <a href="https://arxiv.org/abs/2110.08243" class="btn btn-default btn-sm">Paper</a>
              </div>
            </div>

            <!-- HDMapNet -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/hdmapnet_demo.gif" width="180" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>HDMapNet: An Online HD Map Construction and Evaluation Framework</strong> <span class="label label-danger">Hot</span><br> 
                Qi Li, Yue Wang, Yilun Wang, <strong>Hang Zhao</strong><br>

                <i> "HD map learning from onboard sensors!" </i> <br><br>

                <a href="https://arxiv.org/abs/2107.06307" class="btn btn-default btn-sm">Paper</a>
                <a href="https://tsinghua-mars-lab.github.io/HDMapNet/" class="btn btn-default btn-sm">Project</a>

              </div>
            </div>

            <!-- decorr -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/decorrelation.jpg" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>On Feature Decorrelation in Self-Supervised Learning</strong> <span class="label label-danger">Hot</span><br> 
                Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, <strong>Hang Zhao</strong><br>
                <strong>ICCV 2021 Oral</strong><br>

                <i>"It reveals the connection between model collapse and feature correlations!" </i> <br><br>

                <a href="https://arxiv.org/abs/2105.00470" class="btn btn-default btn-sm">Paper</a>
                <a href="https://tsinghua-mars-lab.github.io/decorr/" class="btn btn-default btn-sm">Project</a>

              </div>
            </div>

            <!-- Motion Dataset -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/waymo_motion.gif" width="250" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>Large Scale Interactive Motion Forecasting for Autonomous Driving: The Waymo Open Motion Dataset</strong><br> 
                Scott Ettinger, et al.<br>
                <strong>ICCV 2021 Oral</strong><br>

                <!-- <i> "Multimodal data brings knowledge for free!" </i> <br><br> -->
                <br>
                <a href="https://arxiv.org/abs/2104.10133" class="btn btn-default btn-sm">Paper</a>
                <a href="https://blog.waymo.com/2021/03/expanding-waymo-open-dataset-with-interactive-scenario-data-and-new-challenges.html" class="btn btn-default btn-sm">Waymo Blog</a>

              </div>
            </div>

            <!-- MKE -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/mke.png" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>Multimodal Knowledge Expansion</strong><br> 
                Zihui Xue, Sucheng Ren, Zhengqi Gao, <strong>Hang Zhao</strong><br>
                <strong>ICCV 2021</strong><br>

                <i> "Multimodal data brings knowledge for free!" </i> <br><br>

                <a href="https://arxiv.org/abs/2103.14431" class="btn btn-default btn-sm">Paper</a>
                <a href="https://tsinghua-mars-lab.github.io/MKE/" class="btn btn-default btn-sm">Project</a>

              </div>
            </div>

            <!-- DenseTNT -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/densetnt.png" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets</strong> <span class="label label-danger">Hot</span><br> 
                Junru Gu, Chen Sun, <strong>Hang Zhao</strong><br>
                <strong>ICCV 2021</strong><br>

                <i> "A SOTA anchor-free and end-to-end multi-trajectory prediction model" </i> <br><br>

                <a href="https://arxiv.org/abs/2108.09640" class="btn btn-default btn-sm">Paper</a>
                <a href="https://arxiv.org/abs/2106.14160" class="btn btn-default btn-sm">Challenge Report</a>
                <a href="https://tsinghua-mars-lab.github.io/DenseTNT/" class="btn btn-default btn-sm">Project</a>

              </div>
            </div>

            <!-- HDMapGen -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/HDMapGen.jpg" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps</strong><br> 
                Lu Mi, <strong>Hang Zhao</strong>, Charlie Nash, Xiaohan Jin, Jiyang Gao, Chen Sun,<br>Cordelia Schmid, Nir Shavit, Yuning Chai, Dragomir Anguelov<br>
                <strong>CVPR 2021</strong><br>

                <!-- <i> "A new SOTA multi-trajectory prediction model." </i> <br><br> -->
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Mi_HDMapGen_A_Hierarchical_Graph_Generative_Model_of_High_Definition_Maps_CVPR_2021_paper.html" class="btn btn-default btn-sm">Paper</a>
              </div>
            </div>

            <!-- SEMI -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/SEMI.jpeg" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>SEMI: Self-supervised Exploration via Multisensory Incongruity</strong><br> 
                Jianren Wang*, Ziwen Zhuang*, <strong>Hang Zhao</strong><br>
                <!-- In Proc. Computer Vision and Pattern Recognition (<strong>CVPR</strong>) 2021<br> -->

                <i> "Multi-sensory incongruity incentizes RL agents to explore!" </i> <br><br>
                <br>
                <a href="https://arxiv.org/abs/2009.12494" class="btn btn-default btn-sm">Paper</a>
              </div>
            </div>

            <!-- TNT -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/tnt.png" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>TNT: Target-driveN Trajectory Prediction</strong> <span class="label label-danger">Hot</span><br> 
                <strong>Hang Zhao</strong>*, Jiyang Gao*, Tian Lan, Chen Sun, Benjamin Sapp,<br> Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai,<br> Cordelia Schmid, Congcong Li, Dragomir Anguelov<br>
                Conference on Robot Learning (<strong>CoRL</strong>) 2020<br>

                <i> "A new motion prediction framework for self-driving!" </i> <br><br>
                <a href="https://arxiv.org/abs/2008.08294" class="btn btn-default btn-sm">Paper</a>
              </div>
            </div>

            <!-- VectorNet -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/Vectornet.gif" width="250" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation</strong> <span class="label label-danger">Hot</span><br> 
                Jiyang Gao*, Chen Sun*, <strong>Hang Zhao</strong>, Yi Shen, <br>
                Dragomir Anguelov, Congcong Li, Cordelia Schmid<br>
                In Proc. Computer Vision and Pattern Recognition (<strong>CVPR</strong>) 2020<br>

                <!-- <i> "A graphical representation of HD maps." </i> <br><br> -->
                <br>
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_VectorNet_Encoding_HD_Maps_and_Agent_Dynamics_From_Vectorized_Representation_CVPR_2020_paper.pdf" class="btn btn-default btn-sm">Paper</a>
                <a href="https://blog.waymo.com/2020/05/vectornet.html" class="btn btn-default btn-sm">Waymo Blog</a>
              </div>
            </div>

            <!-- Waymo Open Dataset -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/waymo_od_challenge.jpg" width="250" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>Scalability in Perception for Autonomous Driving: Waymo Open Dataset</strong><br> 
                Pei Sun et al.<br>
                In Proc. Computer Vision and Pattern Recognition (<strong>CVPR</strong>) 2020<br>
                Seattle (virtual), June. 2020 <br>

                <!-- <i> "Waymo is in a unique position to contribute to the research community with<br> one of the largest and most diverse autonomous driving datasets ever released." </i> <br> -->
                <br>
                <a href="https://waymo.com/open/" class="btn btn-default btn-sm">Project Page</a> 
                <a href="https://waymo.com/open/challenges" class="btn  btn-default btn-sm">Challenges</a> 
                <a href="https://arxiv.org/abs/1912.04838" class="btn btn-default btn-sm">Paper</a> 
                
              </div>
            </div>

            <!-- SoM -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/sound_of_motions.png" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>The Sound of Motions</strong><br> 
                <strong>Hang Zhao</strong>, Chuang Gan, Wei-Chiu Ma, Antonio Torralba<br>
                In Proc. International Conference on Computer Vision (<strong>ICCV</strong>)<br>
                Seoul, Korea, Oct. 2019<br>

                <i> "Listen to the sound of motions!" </i> <br><br>
                <a href="https://arxiv.org/abs/1904.05979" class="btn btn-default btn-sm">Paper (arXiv)</a>
                <a href="videos/SoM_supp.mp4" class="btn btn-default btn-sm">Video</a>
              </div>
            </div>

            <!-- StereoTracking -->
            <!--
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/stereo_tracking.jpeg" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
              <strong> Self-supervised Moving Vehicle Tracking with Stereo Sound</strong><br>
              Chuang Gan, <strong>Hang Zhao</strong>, Peihao Chen, David Cox, Antonio Torralba <br>
              In Proc. International Conference on Computer Vision (<strong>ICCV</strong>)<br>
              Seoul, Korea, Oct. 2019<br>

              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Gan_Self-Supervised_Moving_Vehicle_Tracking_With_Stereo_Sound_ICCV_2019_paper.html" class="btn btn-default btn-sm">Paper</a>

              </div>
            </div>
            -->

            <!-- RFShape -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/rfavatar.gif" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
              <strong>Through-Wall Human Mesh Recovery Using Radio Signals</strong><br>
              Mingmin Zhao, Yingcheng Liu, Aniruddh Raghu, <strong>Hang Zhao</strong>, Tianhong Li, <br>Antonio Torralba, Dina Katabi<br>
              In Proc. International Conference on Computer Vision (<strong>ICCV</strong>)<br>
              Seoul, Korea, Oct. 2019<br>

              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Through-Wall_Human_Mesh_Recovery_Using_Radio_Signals_ICCV_2019_paper.html" class="btn btn-default btn-sm">Paper</a>

              </div>
            </div>

            <!-- HACS -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/slac.jpg" width="200" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
              <strong> HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization</strong><br>
              <strong>Hang Zhao</strong>, Zhicheng Yan, Lorenzo Torresani, Antonio Torralba <br>
              In Proc. International Conference on Computer Vision (<strong>ICCV</strong>)<br>
              Seoul, Korea, Oct. 2019<br>

              <i> "A large-scale dataset for temporal action localization and recognition." </i> <br><br>

              <a href="https://arxiv.org/abs/1712.09374" class="btn btn-default btn-sm">Paper (arXiv)</a>
              <a href="http://hacs.csail.mit.edu" class="btn btn-default btn-sm">Project Page</a>
              <a href="https://github.com/hangzhaomit/HACS-dataset" class="btn btn-default btn-sm">GitHub Page</a>
              </div>
            </div>

            <!-- SoP -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/sound_of_pixels.png" width="220" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
                <strong>The Sound of Pixels</strong> <span class="label label-danger">Hot</span><br> 
                <strong>Hang Zhao</strong>, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, Antonio Torralba<br>
                In Proc. European Conference on Computer Vision (<strong>ECCV</strong>)<br>
                Munich, Germany, Sep. 2018<br>

                <i> "Listen to the sound of pixels!" </i> <br>
                <a href="https://arxiv.org/abs/1804.03160" class="btn btn-default btn-sm">Paper (arXiv)</a>
                <a href="http://sound-of-pixels.csail.mit.edu" class="btn btn-default btn-sm">Project Page</a>
                <a href="https://github.com/hangzhaomit/Sound-of-Pixels" class="btn btn-default btn-sm">Code</a>
                <a href="http://news.mit.edu/2018/ai-editing-music-videos-pixelplayer-csail-0705" class="btn btn-default btn-sm">News Coverage</a>
              </div>
            </div>

            <!-- RFPose3D -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/rf-pose-3d.png" width="250" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
              <strong>RF-Based 3D Skeletons</strong><br>
              Mingmin Zhao, Yonglong Tian, <strong>Hang Zhao</strong>, Mohammad Alsheikh,<br>
              Tianhong Li, Rumen Hristov, Zachary Kabelac, Dina Katabi, Antonio Torralba<br>
              Special Interest Group on Data Communications (<strong>SIGCOMM</strong>) <br>
              Budapest, Hungary, August. 2018 <br>

              <!-- <i> "X-ray vision: pose estimation with radio signals." </i> <br> -->

              <a href="https://dl.acm.org/citation.cfm?id=3230579" class="btn btn-default btn-sm">Paper</a>
              <a href="http://news.mit.edu/2018/artificial-intelligence-senses-people-through-walls-0612" class="btn btn-default btn-sm">News Coverage</a>
              </div>
            </div>

            <!-- RFPose -->
            <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/rf-pose.png" width="250" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
              <strong>Through-Wall Human Pose Estimation Using Radio Signals</strong> <span class="label label-danger">Hot</span><br>
              Mingmin Zhao, Tianhong Li, Mohammad Alsheikh, Yonglong Tian, <strong>Hang Zhao</strong>,<br>Antonio Torralba, Dina Katabi<br>
              In Proc. Computer Vision and Pattern Recognition (<strong>CVPR</strong>) <br>
              Salt Lake City, Utah, June. 2018 <br>

              <!-- <i> "X-ray vision: pose estimation with radio signals." </i> <br> -->

              <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Through-Wall_Human_Pose_CVPR_2018_paper.pdf" class="btn btn-default btn-sm">Paper</a>
              <a href="http://rfpose.csail.mit.edu/" class="btn btn-default btn-sm">Project Page</a>
              <a href="http://news.mit.edu/2018/artificial-intelligence-senses-people-through-walls-0612" class="btn btn-default btn-sm">News Coverage</a>
              </div>
            </div>

            <!-- Open Voc -->
<!--             <div class="container" style="margin-top: 30px;">
              <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
                <img src="images/openvoc.png" width="220" />
              </div>

              <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
              <strong> Open Vocabulary Scene Parsing</strong><br>
              <strong>Hang Zhao</strong>, Xavier Puig, Bolei Zhou, Sanja Fidler, Antonio Torralba <br>
              In Proc. International Conference on Computer Vision (<strong>ICCV</strong>)<br>
              Venice, Italy, Oct. 2017<br>
              arXiv:1703.08769<br>

              <i> "Towards scene parsing with an open and large vocabulary." </i> <br><br>

              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhao_Open_Vocabulary_Scene_ICCV_2017_paper.pdf" class="btn btn-default btn-sm">Paper</a>
              <a href="http://sceneparsing.csail.mit.edu/openvoc/" class="btn btn-default btn-sm">Project Page</a>
              </div>
            </div> -->

            <!-- ADE Challenge 2017-->
            <!--
            <div class="container" style="margin-top: 30px;">
            <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
              <img src="images/challenge2017.jpg" width="220" />
            </div>
            <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
            
            <strong> Places Challenge 2017<br> </strong>
            <i> "I am co-organizing the Joint Workshop for COCO and Places Challenges at ICCV'17.<br>
            New tasks this year: Instance Parsing and Semantic Boundary Detection.<br>
            Check out our dataset now!"</i>
            <br>
            <a href="https://places-coco2017.github.io/" class="btn btn-default btn-sm">Workshop Page</a>
            <a href="http://placeschallenge.csail.mit.edu/" class="btn btn-default btn-sm">Challenge Page</a>
            <a href="https://github.com/CSAILVision/placeschallenge" class="btn btn-default btn-sm">GitHub Page</a>
            <a href="slides/challenge_2017.pdf" class="btn btn-default btn-sm">Slides at ICCV17</a>
            </div>
            </div>
            -->

            <!-- ADE-->
            <div class="container" style="margin-top: 30px;">
            <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
              <img src="images/sceneparsing_cvpr2017.png" width="220" />
            </div>
            <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
            
            <strong>Scene Parsing through ADE20K Dataset</strong> <span class="label label-danger">Hot</span><br>
            Bolei Zhou, <strong>Hang Zhao</strong>, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba <br>
            In Proc. Computer Vision and Pattern Recognition (<strong>CVPR</strong>) <br>
            Honolulu, Hawaii, July. 2017 <br>

              <a href="http://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf" class="btn btn-default btn-sm">Paper</a>
              <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" class="btn btn-default btn-sm">Dataset</a>
              <a href="https://github.com/CSAILVision/sceneparsing" class="btn btn-default btn-sm">Code</a>
              <a href="http://scenesegmentation.csail.mit.edu/" class="btn btn-default btn-sm">Online Demo</a>
              <br>
            </div>
            </div>

            <!-- ADE Challenge 2016-->
            <div class="container" style="margin-top: 30px;">
            <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
              <img src="images/challenge2016.png" width="220" />
            </div>
            <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
            
            <strong> Semantic Understanding of Scenes through the ADE20K Dataset</strong><br>
            Bolei Zhou, <strong>Hang Zhao</strong>, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba <br>
            International Journal on Computer Vision 2018 (<strong>IJCV</strong>)<br>
            <br>

            <strong> ILSVRC'16 MIT Scene Parsing Challenge</strong><br> 
            <i> "I co-organized the scene parsing challenge at ILSVRC'16.
            Check out our dataset now!" <br> </i>
              <a href="http://arxiv.org/abs/1608.05442" class="btn btn-default btn-sm">Paper (arXiv)</a>
              <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" class="btn btn-default btn-sm">Dataset</a>
              <a href="http://sceneparsing.csail.mit.edu/" class="btn btn-default btn-sm">Benchmark Page</a>
              <a href="http://sceneparsing.csail.mit.edu/index_challenge.html" class="btn btn-default btn-sm">Challenge Page</a>
              <a href="https://github.com/CSAILVision/sceneparsing" class="btn btn-default btn-sm">GitHub Page</a>
              <a href="http://scenesegmentation.csail.mit.edu/" class="btn btn-default btn-sm">Online Demo</a>
              <br>
            </div>
            </div>

            <!-- Loss NN -->
            <div class="container" style="margin-top: 30px;">
            <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
              <img src="images/lossNN.png" width="220" />
            </div>
            <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
            <p> <strong> Loss Functions for Neural Networks for Image Processing <span class="label label-danger">Hot</span><br>
            </strong> <strong>Hang Zhao</strong>, Orazio Gallo, Iuri Frosio and Jan Kautz <br>
            arXiv:1511.08861<br>
            IEEE Transactions on Computational Imaging 2017 (<strong>TCI</strong>)<br><br> 

              <i> "How important are loss functions for image processing tasks in deep neural nets?" </i> <br>

              <a href="http://ieeexplore.ieee.org/iel7/6745852/6960042/07797130.pdf" class="btn btn-default btn-sm">Paper (Journal)</a>
              <a href="http://arxiv.org/abs/1511.08861" class="btn btn-default btn-sm">Paper (arXiv)</a>
              <a href="http://research.nvidia.com/publication/loss-functions-image-restoration-neural-networks" class="btn btn-default btn-sm">Project Page</a>
              <a href="https://github.com/NVlabs/PL4NN" class="btn btn-default btn-sm">Code</a>

              <br />
              </p></td>
            </div>
            </div>

            <!-- Duckietown-->
            <div class="container" style="margin-top: 30px;">
            <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
              <img src="images/duckietown.png" width="220" />
          </div>
          <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
            <p> <strong> Duckietown: an Open, Inexpensive and Flexible Platform for Autonomy Education and Research</strong><br>
            
            IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)<br>
            Singapore, May. 2017 <br>

              <i> "We are building an open-source education and research platform for autonomous driving. " </i> <br>
              <a href="papers/duckietown.pdf" class="btn btn-default btn-sm">Paper</a>
              <a href="https://youtu.be/-TwocCeJUe8" class="btn btn-default btn-sm">Video</a>
              <a href="https://duckietown.mit.edu/" class="btn btn-default btn-sm">Project Page</a>
              <a href="https://github.com/duckietown/Software" class="btn btn-default btn-sm">Code</a>
              <a href="http://news.mit.edu/2016/self-driving-cars-meet-rubber-duckies-0420" class="btn btn-default btn-sm">News Coverage</a>

              <br />
              </p></td>
            </div>
            </div>

            <!-- Modulo UHDR -->
            <div class="container" style="margin-top: 30px;">
            <div class="col-xs-12 col-lg-3" style="text-align: center; vertical-align: middle;">
          		<img src="images/moduloHDR.png" width="200" />
       		</div>
       		<div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
       			<p> <strong> Unbounded High Dynamic Range Photography using a Modulo Camera <span class="label label-danger">Hot</span><br> 
            </strong> <strong>Hang Zhao</strong>, Boxin Shi, Christy Fernandez-Cull, Sai-Kit Yeung and Ramesh Raskar <br />
             	In Proc. International Conference on Computational Photography (<strong>ICCP</strong>) <br />
			 	Houston, USA, Apr. 2015 (Acceptance Rate: 24%)<br /> 
			 	Oral Presentation [<strong><a href="images/ICCP2015Award.jpg" style="color:red">Best Paper runner-up</a></strong>] <br><br>
             	<a href="papers/moduloUHDR.pdf" class="btn btn-default btn-sm">Paper</a>
              <a href="posters/ICCP15_Poster.pdf" class="btn btn-default btn-sm">Poster</a>
              <a href="http://web.media.mit.edu/~hangzhao/videos/ICCP15_audio.mp4" class="btn btn-default btn-sm">Video</a>
              <a href="http://web.media.mit.edu/~hangzhao/modulo.html" class="btn btn-default btn-sm">Project Page</a>
              <a href="http://web.media.mit.edu/~hangzhao/modulo.html#news" class="btn btn-default btn-sm">News Coverage</a>
              <br />
           		</p></td>
           	</div>
           	</div>

            <!-- Occluded ToF -->
            <!--
            <div class="container" style="margin-top: 30px;">
            <div class="col-xs-12 col-md-3" style="text-align: center; vertical-align: middle;">
              <img src="images/occluded.png" width="220" />
          </div>
          <div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
            <p><strong>Occluded Imaging with Time of Flight Sensors<br /> </strong>
            <a href="http://web.media.mit.edu/~achoo/">Achuta Kadambi</a>, <strong>Hang Zhao</strong>, Boxin Shi and Ramesh Raskar<br />
              ACM Transactions on Graphics 2015 (<strong>TOG</strong>)<br /> 
              Presented at SIGGRAPH 2016<br /><br />
             <a href="papers/occluded.pdf" class="btn btn-default btn-sm">Paper</a>
             <br>
            </div>
            </div>
            -->

            <!-- Subpixel SR -->
            <!--
            <div class="container" style="margin-top: 30px;">
            <div class="col-xs-12 col-md-3" style="text-align: center; vertical-align: middle;">
          		<img src="images/subpixelSR.png" width="220" />
       		</div>
       		<div class="col-xs-12 col-md-9" style="text-align: left; vertical-align: middle;">
       			<p><strong>Sub-Pixel Layout for Super-Resolution with Images in the Octic Group<br /> </strong>
         		<a href="http://web.media.mit.edu/~shiboxin/">Boxin Shi</a>, <strong>Hang Zhao</strong>, <a href="http://www.ben-ezra.org" target="_blank">Moshe Ben-Ezra</a>, Sai-Kit Yeung, Christy Fernandez-Cull,<br />R. Hamilton Shepard, Christopher Barsi and Ramesh Raskar<br />
             	In Proc. European Conference on Computer Vision (<strong>ECCV</strong>) <br />
			 	Zurich, Switzerland, Sep. 2014 <br /><br />
             <a href="papers/subpixelLayout_SR.pdf" class="btn btn-default btn-sm">Paper</a>
             <a href="posters/ECCV14_Poster.pdf" class="btn btn-default btn-sm">Poster</a>
             <a href="https://youtu.be/Ad1U6VPQh20" class="btn btn-default btn-sm">Video</a>
             <br />
           	</div>
           	</div>
            -->

        </div>
    </div>

    <!-- Teaching -->
    <div class="container" style="margin-top: 0px;">
    <a name="resource" style="visibility: hidden;"></a>
        <div class="col-md-12">
            <br><h2>Teaching</h2>
            <div class="col-md-10" style="text-align: left; vertical-align: middle;">
            <p align="justify"> 
                <li>[Tsinghua] Advances in Autonomous Driving and Intelligent Vehicles (Lecturer)</li>
                <li>[Tsinghua] Introduction to Multimedia Computing (Lecturer)</li>
                <li><a href="http://6.869.csail.mit.edu/fa16/">[MIT 6.869] Advances in Computer Vision (Teaching Assistant)</a></li>
                <li><a href="http://duckietown.mit.edu/">[MIT 2.166] Autonomous Vehicles, also known as "Duckietown" (Course Developer, Teaching Assistant)</a></li>
                <li>[MIT 6.870] Smartphone Vision (Teaching Assistant)</li>
                <li><a href="http://news.mit.edu/2015/2.007-competition-hack-future-0508">[MIT 2.007] Design and Manufacturing I (Teaching Assistant)</a></li>
            </p>
          </div>
        </div>
    </div>

    <!-- Activities -->
    <div class="container" style="margin-top: 0px;">
    <a name="activities" style="visibility: hidden;"></a>
      <div class="col-md-12 content">
        <br /><h2>Professional Activities</h2>
          <div class="col-md-10" style="text-align: middle; vertical-align: middle;">
          <p align="justify"> 

          <li> Co-organizer of <a href="">HACS Temporal Action Localization Challenge</a> at <a href="http://activity-net.org/challenges/2020/">Workshop on International Challenge on Activity Recognition</a> at CVPR 2020.</li>
          <li> Co-organizer of <a href="http://sightsound.org/">Workshop on Sight and Sound</a> at CVPR 2020.</li>
          <li> Co-organizer of <a href="https://sites.google.com/view/multimodalvideo/">Workshop on Multi-modal Video Analysis and Moments in Time Challenge</a> at ICCV 2019.</li>
          <li> Co-organizer of <a href="https://lidchallenge.github.io/">Weakly Supervised Learning for Real-World Computer Vision Applications and the 1st Learning from Imperfect Data (LID) Challenge</a> at CVPR 2019.</li>
          <li> Co-organizer of <a href="http://placeschallenge.csail.mit.edu/">Places Challenge 2017</a>.</li>
          <li> Co-organizer of <a href="https://places-coco2017.github.io/">Joint COCO and Places Recognition Challenge Workshop</a> at ICCV 2017.</li>
          <li> Co-organizer of <a href="http://sceneparsing.csail.mit.edu/index_challenge.html">MIT Scene Parsing Challenge 2016</a>.</li>
          <li> Co-organizer of <a href="http://image-net.org/challenges/LSVRC/2016/index">ILSVRC'16 challenge workshop</a> at ECCV 2016.
          <li>Journal reviewer for TPAMI, IJCV, TIP, CVIU, TCI, OE, etc.</li>   
          <li> Conference reviewer for CVPR, ICCV, ECCV, NIPS, ICML, ICLR, etc.</li>
          <li> Co-chairs of <a href="https://sites.google.com/view/visionseminar">MIT Vision Seminar</a>.</li>
          </li>
          </p>

       </div>
      </div>
    </div>

    <!-- Talks -->
    <div class="container" style="margin-top: 0px;">
    <a name="talks" style="visibility: hidden;"></a>
      <div class="col-md-12 content">
        <br /><h2>Talks</h2>
          <div class="col-md-10" style="text-align: middle; vertical-align: middle;">
          <p align="justify"> 
            <li> Invited talk at Amazon Alexa, May 2019. </li>
            <li> Invited talk at Samsung Workshop at MIT, April 2019. </li>
            <li> Invited talk at Machine Intelligence Conference, March 2019. </li>
            <li> Invited talk at PHILIPS, Feburary 2019. </li>
            <li> Invited talk at VALSE, June 2018. </li>
            <li> Invited talk at Harvard vision seminar, May 2018. </li>
            <li> Invited talk at Google Cambridge, April 2018. </li>
            <li> Invited talk at China Telecom, June 2016. </li>
            <li> Invited talk at MIT graphics seminar, September 2015. </li>
          </p>

       </div>
      </div>
    </div>

    <!-- Resources -->
    <div class="container" style="margin-top: 0px;">
    <a name="resource" style="visibility: hidden;"></a>
        <div class="col-md-12">
            <br /><h2>Resources</h2>
            <div class="col-md-10" style="text-align: left; vertical-align: middle;">
            <p align="justify">
                <li> Open Source Codebase: <a href="https://github.com/CSAILVision/semantic-segmentation-pytorch/">Semantic Segmentation in PyTorch</a></li>

                <li> Poster: <a href="posters/2015Spring_Poster.pdf"> Computational Photography with Novel Camera Sensors</a></li>                
            </p>
          </div>
        </div>
    </div>

    <!-- Publications -->
    <div class="container" style="margin-top: 0px;">
      <a name="publication" style="visibility: hidden;"></a>
        <div class="col-md-12 content">
          <br><h2>Publications</h2>

          <div class="col-md-10" style="text-align: middle; vertical-align: middle;">
          <p align="justify"> 

      [28] <strong>Hang Zhao</strong>*, Jiyang Gao*, Tian Lan, Chen Sun, Benjamin Sapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, Congcong Li, Dragomir Anguelov, "TNT: Target-driveN Trajectory Prediction",  CoRL 2020 (Conference on Robot Learning).<br>
      [27] Jianren Wang, Yujie Lu, <strong>Hang Zhao</strong>, "CLOUD: Contrastive Learning of Unsupervised Dynamics", CoRL 2020 (Conference on Robot Learning).<br>
      [26] Hanhan Li, Ariel Gordon, <strong>Hang Zhao</strong>, Vincent Casser, Anelia Angelova, "Unsupervised Monocular Depth Learning in Dynamic Scenes", CoRL 2020 (Conference on Robot Learning).<br>
      [25] <strong>Hang Zhao</strong>, Chuang Gan, Wei-Chiu Ma, Antonio Torralba, "The Sound of Motions",  ICCV 2019 (International Conference on Computer Vision).<br>
      [24] <strong>Hang Zhao</strong>, Zhicheng Yan, Lorenzo Torresani, Antonio Torralba, "HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization", ICCV 2019 (International Conference on Computer Vision).<br>
      [23] Chuang Gan, <strong>Hang Zhao</strong>, Peihao Chen, David Cox, Antonio Torralba, "Self-supervised Moving Vehicle Tracking with Stereo Sound", ICCV 2019 (International Conference on Computer Vision).<br>
      [22] Mingmin Zhao, Yingcheng Liu, Aniruddh Raghu, Hang Zhao, Tianhong Li, Antonio Torralba, Dina Katabi, "Through-Wall Human Mesh Recovery Using Radio Signals", ICCV 2019 (International Conference on Computer Vision).<br>
      [21] Andrew Rouditchenko*, <strong>Hang Zhao</strong>*, Chuang Gan, Josh McDermott, Antonio Torralba, “Self-Supervised Audio-visual Co-segmentation”, ICASSP 2019 (International Conference on Acoustics, Speech and Signal Processing).<br>
      [20] <strong>Hang Zhao</strong>, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, Antonio Torralba, “The Sound of Pixels”, ECCV 2018 (European Conference on Computer Vision), <a href="http://arxiv.org/abs/1804.03160">arXiv:1804.03160</a>.<br>
      [19] Mingmin Zhao, Yonglong Tian, <strong>Hang Zhao</strong>, Mohammad Alsheikh, Tianhong Li, Rumen Hristov, Zachary Kabelac, Dina Katabi, Antonio Torralba, “RF-Based 3D Skeletons”, SIGCOMM 2018 (Special Interest Group on Data Communications). <br>
      [18] Mingmin Zhao, Tianhong Li, Mohammad Alsheikh, Yonglong Tian, <strong>Hang Zhao</strong>, Antonio Torralba, Dina Katabi, "Through-Wall Human Pose Estimation Using Radio Signals", CVPR 2018 (Computer Vision and Pattern Recognition).<br>
      [17] <strong>Hang Zhao</strong>, Xavier Puig, Bolei Zhou, Sanja Fidler, Antonio Torralba, “Open vocabulary scene parsing”, ICCV 2017 (International Conference on Computer Vision), <a href="http://arxiv.org/abs/1703.08769">arXiv:1703.08769</a>.<br>
      [16] Bolei Zhou, <strong>Hang Zhao</strong>, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba, “Scene Parsing through ADE20K Dataset”, CVPR 2017 (Computer Vision and Pattern Recognition).<br />
      [15] Bolei Zhou, <strong>Hang Zhao</strong>, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba, “Semantic understanding of scenes through the ADE20K dataset”, <a href="http://arxiv.org/abs/1608.05442">arXiv:1608.05442</a>.<br>
      [14] Liam Paul, et al., “Duckietown: an open, inexpensive and flexible platform for autonomy education and research”, ICRA 2017 (International Conference on Robotics and Automation).<br />
      [13] <strong>Hang Zhao</strong>, Orazio Gallo, Iuri Frosio and Jan Kautz, “Loss functions for neural networks for image processing”, IEEE Transactions on Computational Imaging 2017 (TCI), <a href="http://arxiv.org/abs/1511.08861">arXiv:1511.08861</a>.<br>
      [12] <strong>Hang Zhao</strong>, Boxin Shi, Christy Fernandez-Cull, Sai-Kit Yeung and Ramesh Raskar, “Unbounded high dynamic range photography using a modulo camera”, ICCP 2015 (International Conference on Computational Photography).<br>
      [11] Christy Fernandez-Cull, <strong>Hang Zhao</strong>, Boxin Shi, Brian M. Tyrrell, Joseph Lin and Ramesh Raskar, “Snapshot on-chip HDR ROIC architectures”, COSI 2015 (Computational Optical Sensing and Imaging).<br>
      [10]  Achuta Kadambi, <strong>Hang Zhao</strong>, Boxin Shi and Ramesh Raskar, “Occluded imaging with time of flight sensors”, ACM Transactions on Graphics 2015.<br>
      [9] Boxin Shi, <strong>Hang Zhao</strong>, Moshe Ben-Ezra, et al., “Sub-pixel layout for super-resolution with images in an octic group”, ECCV 2014 (European Conference on Computer Vision).<br>
      [8] Hamilton Shepard, Christy Fernandez-Cull, Ramesh Raskar, Boxin Shi, Christopher Barsi and <strong>Hang Zhao</strong>, “Optical design and characterization of an advanced computational imaging system”, SPIE 2014.<br />
      [7] <strong>Hang Zhao</strong>, Yuanqing Yang, Qiang Li and Min Qiu, “Sub-wavelength quarter-wave plate based on plasmonic patch antennas”, Applied Physics Letters. 2013, 103, 261108.<br />
      [6] Jun Zhu, <strong>Hang Zhao</strong> and Min Qiu, “Surface waves on the relativistic quantum plasma half-space”. Physics Letter A. 2013, 377, 1736-1739.<br />
      [5] Qing Wang, <strong>Hang Zhao</strong>, Xu Du, Weichun Zhang, Qiang Li and Min Qiu, “Hybrid photonic- plasmonic molecule based on metal/Si disks”, Optics Express. 2013. 21, 11037-11047.<br />
      [4] Qiang Li, Weichun Zhang, <strong>Hang Zhao</strong> and Min Qiu, “Two-dimensional analysis photothermal properties in nanoscale plasmonic waveguides for optical interconnect”, Journal of Lightwave Technology. 2013, 31, 4051-4056.<br />
      [3] <strong>Hang Zhao</strong>, Shengtao Mei and Jianqi Shen, “Influence of vapor pressure and Doppler effect on the optical property of a coherent atomic vapor”. Applied Physics, 2013, 3, 18-25.<br />
      [2] Shengtao Mei, <strong>Hang Zhao</strong> and Jianqi Shen, “Tunable quantum interference in a four-level atomic system for photonic device design”. The 33rd Progress in Electromagnetics Research Symposium, Taipei, 2013.<br />
      [1] Qiang Li, <strong>Hang Zhao</strong>, Jie Tian, Jing Wang, Yi Song and Min Qiu, “Plasmonic devices for optical interconnect”, The 11th International Conference on Optical Communications and Networks, Thailand, 2012. 
      </p>

       </div>
      </div>
    </div>

    <!-- Affiliations -->
    <div class="container" style="margin-top: 30px;">
    	<div class="col-md-12">
            <h3>Current and Past Affiliations</h3>
        </div>
        <div class="container">
     	  <div class="col-xs-6 col-md-2" style="text-align: center; align: middle;">
            <a href="https://www.tsinghua.edu.cn/en/"> <img src="images/tsinghua.jpg" height="95" /> </a>
        </div>
        <div class="col-xs-6 col-md-2" style="text-align: center; align: middle;">
            <a href="http://csail.mit.edu"> <img src="images/csail_logo.png" height="95" /> </a>
        </div>
        <div class="col-xs-6 col-md-2" style="text-align: center; align: middle;">
          	<a href="http://media.mit.edu"> <img src="images/MIT_ML_Logo.png" height="95" /> </a>
        </div>
        <!--  <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
          	<a href="http://http://meche.mit.edu"> <img src="images/MIT_ME_Logo.jpeg" height="100" /> </a>
        </div> -->
        <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
          	<a href="http://www.mit.edu"> <img src="images/MIT_Logo.png" height="75" /> </a>
        </div>
        <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
          	<a href="http://www.ucla.edu/"> <img src="images/UCLA_Logo.jpg" height="85" /> </a>
        </div> 
        <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
          	<a href="http://www.zju.edu.cn/english/"> <img src="images/ZJU_Logo.png" height="100" /> </a>
        </div>

        <!-- Company -->
        <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
            <a href="https://research.fb.com/"> <img class="img-center" src="images/waymo_logo.png" height="90" > </a>
        </div>
          <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
            <a href="https://research.fb.com/"> <img class="img-center" src="images/facebook_logo.jpg" height="110" > </a>
        </div>
        <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
            <a href="http://www.merl.com/"> <img class="img-center" src="images/MERL_logo.png" height="75" > </a>
        </div>
        <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
          	<a href="http://www.nvidia.com"> <img class="img-center" src="images/NVIDIA_Logo.png" height="90" > </a>
       	</div>
        <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
            <a href="http://www.tvisioninsights.com"> <img src="images/TVI_Logo.jpg" height="80" /> </a>
        </div>
        <!-- <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
            <a href="http://www.upm.es/"> <img src="images/UPM_Logo.gif" height="95" /> </a>
        </div> -->
        <!-- <div class="col-xs-6 col-md-2" style="text-align: center; vertical-align: middle;">
          	<a href="http://www.kuang-chi.org"> <img src="images/KC_Logo.jpg" height="100" /> </a>
       	</div> -->
       	</div>
    </div>


    <!-- Statistics
    <div class="container" style="margin-top: 30px;">
      	<div class="page-header" style="margin-top: 0px;"></div>
		<div class="col-md-2">
			<a href="http://info.flagcounter.com/a6Wx"><img src="http://s06.flagcounter.com/count/a6Wx/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_8/viewers_Welcome/labels_0/pageviews_0/flags_0/" alt="Flag Counter" border="0"></a>
        </div>

    </div>
    -->
</div>

        <footer>
            <div class="container">
                <ul class="list-unstyled">
                    <li class="pull-right"><a href="#top">Back to top</a></li>
                </ul>
            </div>
            <div style="text-align: center">
                <span>©
                    <script>document.write(new Date().getFullYear())</script>
                    Hang Zhao 
                </span>
            </div>
        </footer>

	</body>
</html>
